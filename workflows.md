---
layout: page
title: Workflows in Detail
permalink: /workflows/
---

<h1>Pipeline details</h1>

Raw paired-end reads are merged using SEQPREP v1.3.2 from bioconda (St. John, 2016).  This step looks for a minimum Phred quality score of 13 in the overlap region, requires at least a 25bp overlap.  These paramters are adjustable.  Using Phred quality cutoff of 13 at this step is actually more stringent than using a Phred quality score cutoff of 20 at this step as more bases will exceed the cutoff when aligning the paired reads and more mismatches (if present) are counted.

Primers are trimmed in two steps using CUTADAPT v3.2 from bioconda (Martin, 2011).  This step now uses the linked adapter approach to remove forward and reverse primers in one step.  Primer sequences need to be specified in an adapters.fasta file and the user may wish to anchor them or not, see CUTADAPT manual for details https://cutadapt.readthedocs.io/en/stable/guide.html?highlight=linked#linked-adapters .  At this step, CUTADAPT looks for a minimum Phred quality score of at least 20 at the ends, no more than 10% errors allowed in the primer, no more than 3 N's allowed in the rest of the sequence, trimmed reads need to be at least 150 bp, untrimmed reads are discarded.  Each of these parameters are adjustable. 

Files are reformatted and samples are combined for a global analysis.

Reads are dereplicated (only unique sequences are retained) using VSEARCH v2.15.2 from bioconda (Rognes et al., 2016).

Zero-radius OTUS (Zotus) are generated using VSEARCH with the unoise3 algorithm (Edgar, 2016).  They are similar to OTUs delimited by 100% sequence similarity, except that they are also error-corrected and rare clusters are removed.  Here, we define rare sequences to be sequence clusters containing only one or two reads (singletons and doubletons) and these are also removed as 'noise'.  We refer to these sequence clusters as exact sequence variants (ESVs) for clarity.  For comparison, the error-corrected sequence clusters generated by DADA2 are referred to as either amplicon sequence variants (ASVs) or ESVs in the literature (Callahan et al., 2016).  Putative chimeric sequences are then removed using the uchime3_denovo algorithm in VSEARCH.

An ESV x sample table that tracks read number for each ESV is generated with VSEARCH using --search_exact .  Note that this in this pipeline this is just an intermediate file and that the final retained set of ESVs and mapped read numbers should be retrieved from the final output file (results.csv).

For ITS, the ITSx extractor is used to remove flanking rRNA gene sequences so that subsequent analysis focuses on just the ITS1 or ITS2 spacer regions (Bengtsson-Palme et al., 2013).

For the standard pipeline (ideal for rRNA genes) performs taxonomic assignments using the Ribosomal Database classifier v2.13 (RDP classifier).  We have provided a list of RDP-trained classifiers that can be used with MetaWorks (Table 1). 

**Table 1.  Where to download trained RDP classifiers for a variety of popular marker gene/metabarcoding targets.**

<table class="table table-hover">
  <thead>
    <tr>
      <th scope="col">Marker</th>
      <th scope="col">Target Taxa</th>
      <th scope="col">Classifier Availability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">COI</th>
      <td>Eukaryotes</td>
      <td><a href="https://github.com/terrimporter/CO1Classifier">CO1 Classifier</a></td>
    </tr>
    <tr>
      <th scope="row">rbcL</th>
      <td>Diatoms</td>
      <td><a href="https://github.com/terrimporter/rbcLdiatomClassifier">rbcL Diatom Classifier</a></td>
    </tr>
    <tr>
      <th scope="row">rbcL</th>
      <td>Eukaryotes</td>
      <td><a href="https://github.com/terrimporter/rbcLClassifier">rbcL Classifier</a></td>
    </tr>
    <tr>
      <th scope="row">rbcL</th>
      <td>Land Plants</td>
      <td><a href="https://github.com/terrimporter/rbcL_landPlant_Classifier">rbcL Land Plant Classifier</a></td>
    </tr>
    <tr>
      <th scope="row">12S</th>
      <td>Fish</td>
      <td><a href="https://github.com/terrimporter/12SfishClassifier">12S Fish Classifier</a></td>
    </tr>
    <tr>
      <th scope="row">12S</th>
      <td>Vertebrates</td>
      <td><a href="https://github.com/terrimporter/12SvertebrateClassifier">12S Vertebrate Classifier</a></td>
    </tr>
    <tr>
      <th scope="row">SSU (18S)</th>
      <td>Diatoms</td>
      <td><a href="https://github.com/terrimporter/SSUdiatomClassifier">SSU Diatom Classifier</a></td>
    </tr>
    <tr>
      <th scope="row">SSU (16S)</th>
      <td>Vertebrates</td>
      <td><a href="https://github.com/terrimporter/16SvertebrateClassifier">16S Vertebrate Classifier</a></td>
    </tr>
    <tr>
      <th scope="row">SSU (18S)</th>
      <td>Eukaryotes</td>
      <td><a href="https://github.com/terrimporter/18SClassifier">18S Classifier</a></td>
    </tr>
    <tr>
      <th scope="row">SSU (16S)</th>
      <td>Prokaryotes</td>
      <td>Built-in to the RDP Classifier</td>
    </tr>
    <tr>
      <th scope="row">ITS</th>
      <td>Fungi (Warcup)</td>
      <td>Built-in to the RDP Classifier</td>
    </tr>
    <tr>
      <th scope="row">ITS</th>
      <td>Fungi (UNITE 2014)</td>
      <td>Built-in to the RDP Classifier</td>
    </tr>
      <tr>
      <th scope="row">ITS</th>
      <td>Fungi (UNITE 2021)</td>
      <td><a href="https://github.com/terrimporter/UNITE_ITSClassifier">UNITE ITS Classifier</a></td>
    </tr>
    <tr>
      <th scope="row">ITS</th>
      <td>Plants (PLANiTS)</td>
      <td><a href="https://github.com/terrimporter/PLANiTS_ITSClassifier">PLANiTS ITS Classifier</a></td>
    </tr>
    <tr>
      <th scope="row">LSU</th>
      <td>Fungi</td>
      <td>Built-in to the RDP Classifier</td>
    </tr>
  </tbody>
</table>

If you are using the pipeline on a protein coding marker that does not yet have a HMM.profile, such as rbcL, then ESVs are translated into every possible open reading frame (ORF) on the plus strand using ORFfinder v0.4.3 (Porter and Hajibabae, 2021).  The longest ORF (nt) is reatined for each ESV.  Putative pseudogenes are removed as outliers with unusually small/large ORF lengths.  Outliers are calcualted as follows:  Sequence lengths shorter than the 25th percentile - 1.5\*IQR (inter quartile range) are removed as putative pseudogenes (ore sequences with errors that cause a frame-shift).  Sequence lengths longer than the 75th percentile + 1.5\*IQR are also removed as putative pseudogenes.

If you use the pipeline on a protein coding marker that has a HMM.profile, such as COI arthropoda, then the ESVs are translated into nucleotide and amino acid ORFs using ORFfinder, the longest orfs are retained and consolidated.  Amino acid sequences for the longest ORFs are used for profile hidden Markov model sequence analysis using HMMER v3.3.2.  Sequence bit score outliers are identified as follows: ORFs with scores lower than the 25th percentile - 1.5\*IQR (inter quartile range) are removed as putative pseudogenes.  This method should remove sequences that don't match well to a profile HMM based on arthropod COI barcode sequences mined from public data at BOLD.

The final output file is results.csv and it has been formatted to specify ESVs from each sample, read counts, ESV/ORF sequences, and column headers.  Additional statistics and log files are also provided for each major bioinformatic step.

*Note 1*: If you choose to further cluster denoised ESVs into OTUs, then this is done using the cluster_smallmem method in VSEARCH with id set to 0.97.  Primer trimmed reads are then mapped to OTUs to create an OTU table using the usearch_global method with id set to 0.97.  The remainder of the pipeline proceeds as described above.  The final output file is results_OTU.csv.

*Note 2*: If you have samples sequenced at diffrent times (multiple seasons, years, or trials), you will likely process these samples right after sequencing resulting in multiple sets of ESVs.  To facilitate downstream processing, it may be advantagous to have a GLOBAL set of ESV ids so that data can be compared at the ESV level accross seasons, years, or trials.  The directories containing the output files processed using the default pipeline need to be in the same directory as the snakefile script. Ex. 16S_trial1, 16S_trial2, 16S_trial3.  In each of these directoreis, there is a cat.denoised.nonchimeras file and a results.csv file.  The denoised (chimera-free) ESVs (or ITSx processed ESVs) are concatenated into a single file, dereplicated, relabelled using the SHA1 method.  This file then becomes the new global ESV sequence database.  A fasta file is generated from the results.csv file and these sequences are clustered with the new global ESV sequence database using the usearch_global methods with the id set to 1.0.  The global ESV that each trial ESV clusters with is parsed and mapped to the final output file called global_results.csv.

*Note 3*: If you have samples sequenced at diffrent times (multiple seasons, years, or trials), you will likely process these samples right after sequencing resulting in multiple sets of ESVs.  To facilitate downstream processing, it may be advantagous to have a GLOBAL set of OTU ids so that data can be compared at the ESV level accross seasons, years, or trials.  The directories containing the output files processed using the default pipeline need to be in the same directory as the snakefile script. Ex. 16S_trial1, 16S_trial2, 16S_trial3.  In each of these directoreis, there is a cat.denoised.nonchimeras file and a results.csv file.  The denoised (chimera-free) ESVs (or ITSx processed ESVs) are concatenated into a single file, dereplicated, relabelled using the SHA1 method, then clustered into OTUs with 97% sequence similarity.  This file then becomes the new global OTU sequence database.  A fasta file is generated from the results.csv file and these sequences are clustered with the new global OTU sequence database using the usearch_global methods with the id set to 0.97 .  The global OTU that each trial ESV clusters with is parsed and mapped to the final output file called global_results.csv.

*Note 4*: If you are using the single read pipeline, the read pairing step with SeqPrep is skipped.  If processing the R1 read, raw read statistics are calculated, then the primer is trimmed using CUTADAPT as described above.  If processing the R2 read, raw read statistics are calculated then the read is reverse-complemented before the primer is trimmed using CUTADAPT as described above.  The final file is results.csv.  When assessing the quality of your taxonomic assignments, be sure to use the appropriate bootstrap support cutoffs for these shorter than usual sequences.

*Note 5*: If you are sorting out tagged individuals, it is advised to set the CUTADAPT -O parameter to maximize the tag+primer overlap with the read.  For example, if the range of tag+primer lengths is 25 - 27bp, then set -O 25 (default is 3 bp).  If the tags are short, then it is especially important to ensure no mismatches between this and the read.  For this reason, it is recommended to set the CUTADAPT -e parameter to allow for zero errors between the tag+primer and read, -e 0 (default is 0.1 or 10% errors).

## Prepare your environment to run the pipeline

1. This pipeline includes a conda environment that provides most of the programs needed to run this pipeline (SNAKEMAKE, SEQPREP, CUTADAPT, VSEARCH, etc.).

```linux
# Create the environment from the provided environment.yml file.  Only need to do this step once.
conda env create -f environment.yml

# Activate the environment
conda activate MetaWorks_v1.10.0

# On the GPSC activate using source
source ~/miniconda/bin/activate MetaWorks_v1.10.0
```

2. The RDP classifier comes with the training sets to classify 16S, fungal LSU or ITS rDNA.  To classify other markers using custom-trained RDP sets, obtain these from GitHub using Table 1 as a guide .  Take note of where the rRNAclassifier.properties file is as this needs to be added to the config.yaml .

```linux
RDP:
    t: "/path/to/CO1Classifier/v4/mydata_trained/rRNAClassifier.properties"
```

3. If doing pseudogene filtering, then download and install the NCBI ORFfinder

The pipeline requires ORFfinder 0.4.3 available from the NCBI at ftp://ftp.ncbi.nlm.nih.gov/genomes/TOOLS/ORFfinder/linux-i64/ .  This program should be downloaded, made executable, and put in your conda environment path (ex. ~/miniconda/envs/MetaWorks_v1.10.0/bin).

```linux
# go to your conda environment bin
cd ~/miniconda3/envs/MetaWorks_v1.10.0/bin/.

# download ORFfinder
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/TOOLS/ORFfinder/linux-i64/ORFfinder.gz

# decompress
gunzip ORFfinder.gz

# make executable
chmod a+x ORFfinder
```

Run the program to test that it works:

```linux
ORFfinder
```

If you get an error that requries newer GLIBC libraries (libc.so.6) or if you get an error that requires libnghttp2 then follow the instructions at [Add libraries for ORFfinder](#add-libraries-for-orffinder).

4. In most cases, your raw paired-end Illumina reads can go into a directory called 'data' which should be placed in the same directory as the other files that come with this pipeline.

```linux
# Create a new directory to hold your raw data
mkdir data
```

5. Please go through the config.yaml file and edit directory names, filename patterns, etc. as necessary to work with your filenames.

6. Run Snakemake by indicating the number of jobs or cores that are available to run the whole pipeline.  

```linux
snakemake --jobs 24 --snakefile snakefile --configfile config.yaml
```

7. When you are done, deactivate the conda environment:

```linux
conda deactivate
```

## Quick start example using COI test data

This MetaWorks quick start assumes that you have already installed the CO1 Classifier available from https://github.com/terrimporter/CO1Classifier .  If you have not already done so, load the CO1 Classifier by following the quickstart instructions on the CO1 Classifier GitHub page.  It also assumes that you already have conda installed, otherwise see [Prepare your environment to run the pipeline](#prepare-your-environment-to-run-the-pipeline).

```linux
# download latest version of MetaWorks
wget https://github.com/terrimporter/MetaWorks/releases/download/v1.10.0/MetaWorks1.9.5.zip
unzip MetaWorks1.9.5.zip
cd MetaWorks1.9.5

# edit config_testing_COI_data.yaml file to customize path to CO1v4 classifier properties file (line 131)

# create the latest conda environment and activate it
conda env create -f environment.yml
conda activate MetaWorks_v1.10.0

# run the pipeline on the COI test data
snakemake --jobs 1 --configfile config_testing_COI_data.yaml --snakefile snakefile_ESV
```

Once you have the results.csv file, results can be imported into R for further analysis.  If you need an ESV table for downstream analysis this can be generated in R as well [How to create an ESV table](#how-to-create-an-esv-table).